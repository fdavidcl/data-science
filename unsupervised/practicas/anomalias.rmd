---
title: "Anomalías"
subtitle: "Aprendizaje no supervisado y detección de anomalías - Máster DATCOM"
author: "Francisco David Charte Luque"
date: "Universidad de Granada"
output: 
  pdf_document: 
    latex_engine: xelatex
toc: true
numbersections: true
monofont: "Fira Code"
lang: es-ES
header-includes:
  - \usepackage{subcaption}
  - \usepackage{wrapfig}
  - \usepackage{hyperref}
  - \def\figureautorefname{Figura }
---
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = T,
  fig.keep = "all",
  out.width = "0.6\\textwidth",
  fig.width = 6,
  out.height = "0.4\\textwidth",
  fig.height = 4
)
options(width = 100)

library(magrittr)
source("!Outliers_A2_Librerias_a_cargar_en_cada_sesion.R")
source("!Outliers_A3_Funciones_a_cargar_en_cada_sesion.R")
X11 = function(){}
```

\clearpage

# Descripción del conjunto de datos

El dataset \textit{abalone}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Abalone}} abarca diferentes medidas físicas de conchas de abulón (\autoref{fig:abalone}) provinientes de Tasmania, y su objetivo es predecir la edad de la concha. El método experto para determinar la edad consiste en cortar y tintar la concha, para después contar el número de anillos mediante un microscopio.

\begin{wrapfigure}{r}{0.3\textwidth}
  \centering
  \includegraphics[width=0.3\textwidth]{haliotis.jpg}
  \caption{\label{fig:abalone}Concha de abulón \textit{Haliotis rubra}. Imagen de Peter Southwood/Wikimedia Commons (CC BY-SA).}
\end{wrapfigure}

Se trata de un dataset de regresión cuya variable objetivo no es realmente la edad de cada individuo, sino el número de anillos, \textit{rings}. Sumando 1.5 a este número se puede obtener la edad en años. Esta variable oscila entre 1 y 29, con la mitad de las conchas presentando entre 8 y 11 anillos.

\textit{abalone} comprende 4177 instancias y 9 variables, de las cuales una es nominal y el resto numéricas:
\begin{enumerate}
\item \textit{sex}: el sexo del abulón, con 3 posibles valores: \textit{male}, \textit{female} e \textit{infant}.
\item \textit{length}: longitud máxima en milímetros.
\item \textit{diameter}: longitud perpendicular a la máxima en milímetros.
\item \textit{height}: altura de la concha en milímetros.
\item \textit{whole}: peso completo del individuo en gramos.
\item \textit{shucked}: peso de la carne en gramos.
\item \textit{viscera}: peso de las vísceras en gramos.
\item \textit{shell}: peso de la concha tras secar en gramos.
\item \textit{rings}: número de anillos de la concha
\end{enumerate}

A continuación cargamos el dataset desde su URL original y ajustamos los nombres de las variables:

```{r load}
origin <- "https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"
abalone <- read.csv(origin, header = FALSE)
names(abalone) <- c(
  "sex", "length", "diameter", "height", 
  "whole", "shucked", "viscera", "shell", "rings"
)
```

# Anomalías univariantes: rango intercuartílico

Para el análisis de anomalías univariantes selecciono la columna novena (*rings*):

```{r uni}
mydata.numeric <- abalone[, 2:9]
nombre.mydata  <- "abalone"
indice.columna <- 8

mydata.numeric.scaled <- as.data.frame(lapply(mydata.numeric, scale))
rownames(mydata.numeric.scaled) <- rownames(mydata.numeric)
columna <- mydata.numeric[, indice.columna]
nombre.columna <- names(mydata.numeric)[indice.columna]
columna.scaled <- mydata.numeric.scaled[, indice.columna]
```

## Cómputo de los outliers IQR

Calculamos los cuartiles y la distancia intercuartílica. Además, establecemos los límites a partir de los cuales consideramos que un elemento es outlier normal y extremo:

```{r iqr, echo = c(-4,-10)}
cuartil.primero <- quantile(columna, 1/4)
cuartil.tercero <- quantile(columna, 3/4)
iqr <- IQR(columna)
cat("Q1:", cuartil.primero, "| Q3:", cuartil.tercero, "| IQR:", iqr)

extremo.superior.outlier.normal  <- cuartil.tercero + 1.5 * iqr
extremo.inferior.outlier.normal  <- cuartil.primero - 1.5 * iqr
extremo.superior.outlier.extremo <- cuartil.tercero + 3 * iqr
extremo.inferior.outlier.extremo <- cuartil.primero - 3 * iqr
cat(extremo.superior.outlier.normal, "|", extremo.inferior.outlier.normal, "|", extremo.superior.outlier.extremo, "|", extremo.inferior.outlier.extremo)

vector.es.outlier.normal  <-
  columna > extremo.superior.outlier.normal  |
  columna < extremo.inferior.outlier.normal
vector.es.outlier.extremo <-
  columna > extremo.superior.outlier.extremo |
  columna < extremo.inferior.outlier.extremo
```

Consideraremos que un ejemplo es outlier normal si supera los 15.5 anillos o es inferior a 3.5, y que es extremo si supera los 20. Es destacable que, al tratarse de una medida estrictamente positiva, no habrá ejemplos con menos de -1 anillos. Además, habrá ejemplares jóvenes de la población que tendrán menos de 3.5 anillos y posiblemente no deberíamos considerarlos anómalos, ya que su única diferencia con los demás sería la edad.

Esta técnica encuentra `r sum(vector.es.outlier.normal)` outliers *normales* y `r sum(vector.es.outlier.extremo)` outliers *extremos*.

## Índices y valores de los outliers

```{r iqrout}
claves.outliers.normales     <- which(vector.es.outlier.normal)
data.frame.outliers.normales <- mydata.numeric[claves.outliers.normales, ]
nombres.outliers.normales    <- rownames(data.frame.outliers.normales)
valores.outliers.normales    <- data.frame.outliers.normales[, indice.columna]

claves.outliers.extremos     <- which(vector.es.outlier.extremo)
data.frame.outliers.extremos <- mydata.numeric[claves.outliers.extremos, ]
nombres.outliers.extremos    <- rownames(data.frame.outliers.extremos)
valores.outliers.extremos    <- data.frame.outliers.extremos[, indice.columna]
print(data.frame.outliers.normales[1:10, ])
print(data.frame.outliers.extremos[1:10, ])
```

No muestro los resultados completos por cuestiones de espacio. Observamos que los outliers son en su mayoría ejemplos que exceden el umbral superior de anillos, y sólo `r sum(valores.outliers.normales < extremo.inferior.outlier.normal)` del total de  `r sum(vector.es.outlier.normal)` outliers se quedan por debajo del umbral inferior.

## Desviación de los outliers con respecto a la media de la columna

Extraigo los valores normalizados:

```{r iqrd}
valores.normalizados.outliers.normales <- columna.scaled[claves.outliers.normales]
```

## Plot

```{r iqrp}
MiPlot_Univariate_Outliers(columna, claves.outliers.normales, nombre.columna)
MiPlot_Univariate_Outliers(columna, claves.outliers.extremos, nombre.columna)
```

Estos plots nos muestran los ejemplos que quedan etiquetados como outliers al establecer umbrales inferior y superior. Observamos que una gran cantidad de ellos se han considerado outliers normales, cuando tal vez podríamos pensar que la *edad* de esta especie puede tener una cola larga a la derecha, habiendo pocos individuos longevos pero entrando dentro de lo habitual. Los outliers extremos sí muestran mayor separación del resto, habiendo muy pocos ejemplos que tengan tal número de anillos, denotando ejemplares especialmente longevos.

## Boxplot

```{r iqrb, out.width = "0.3\\textwidth", fig.width = 3, out.height = "0.5\\textwidth", fig.height = 5}
MiBoxPlot_IQR_Univariate_Outliers(mydata.numeric, indice.columna, coef = 1.5)
MiBoxPlot_IQR_Univariate_Outliers(mydata.numeric.scaled, indice.columna, coef = 1.5)
```

Comprobamos que los boxplots no varían al usar los datos normalizados. Se nos muestran los datos anómalos según el IQR etiquetados con su índice, al haber tantos se solapan y sólo se pueden apreciar algunos de los extremos. Notamos que la mayor parte de la población se condensa en una banda relativamente estrecha de número de anillos.

## Cómputo con funciones propias

```{r iqrprop}
vector.es.outlier.normal  <- vector_es_outlier_IQR(mydata.numeric, indice.columna, coef = 1.5)
vector.es.outlier.extremo <- vector_es_outlier_IQR(mydata.numeric, indice.columna, coef = 3)
claves.outliers.normales <- vector_claves_outliers_IQR(mydata.numeric, indice.columna, coef = 1.5)
claves.outliers.extremos <- vector_claves_outliers_IQR(mydata.numeric, indice.columna, coef = 3)
```

Comprobamos que, efectivamente, las funciones dan los mismos resultados:

- número de outliers normales: `r sum(vector.es.outlier.normal)`
- número de outliers extremos: `r sum(vector.es.outlier.extremo)`

## Trabajamos con varias columnas

Ahora buscamos outliers mediante el rango intercuartílico en cualquier columna:

```{r iqrvarias}
indices.de.outliers.en.alguna.columna <-
  vector_claves_outliers_IQR_en_alguna_columna(mydata.numeric, coef = 1.5)
```

En este caso se encuentran `r length(indices.de.outliers.en.alguna.columna)` outliers univariantes en total.

## Ampliación: Índices y valores de los outliers

```{r iqrfr}
frame.es.outlier <- sapply(1:ncol(mydata.numeric), function(col)
  vector_es_outlier_IQR(mydata.numeric, col)
)
numero.total.outliers.por.columna <- structure(
  colSums(frame.es.outlier),
  names = colnames(mydata.numeric)
)
print(numero.total.outliers.por.columna)
indices.de.outliers.en.alguna.columna <- 
  1:ncol(mydata.numeric) %>% 
  lapply(function(col) vector_claves_outliers_IQR(mydata.numeric, col, coef = 1.5)) %>%
  unlist %>% unique
```

Lo que observamos en el resultado es que hay muchos más datos anómalos en cuanto a número de anillos (278) que en el resto de variables, que hay entre 26 y 59. Esto puede denotar que habrá ejemplos que, pese a que tengan tamaño y peso *normales*, tendrán un número de anillos muy superior a lo común.

Esta versión de la búsqueda de outliers en alguna columna elimina repeticiones, así que encuentra `r length(indices.de.outliers.en.alguna.columna)` instancias que tienen al menos un outlier, formando un `r 100 * length(indices.de.outliers.en.alguna.columna) / nrow(abalone)`% de los ejemplos.


## Ampliación: Desviación de los outliers con respecto a la media de la columna

```{r iqrany}
mydata.numeric.scaled[indices.de.outliers.en.alguna.columna[1:15], ]
```

No muestro los datos completos por cuestión de espacio. En estos ejemplos, observamos que muchos casos de anomalías en columnas distintas de *rings* resultan ser por debajo de la media. En particular, estos ejemplos son anómalos para el atributo *length*.

## Ampliación: Boxplot

```{r iqrbox}
boxplot(mydata.numeric.scaled)
MiBoxPlot_juntos(mydata.numeric)
```

En estos gráficos vemos que la columna *height* tiene dos anomalías muy definidas y separadas del resto. Las columnas relacionadas con el tamaño físico del espécimen tienen mayoría de outliers por debajo de la media, mientras que los anillos y las medidas de peso tienen los outliers por encima.

<!--
## Tests estadísticos

## Test de Grubbs

```{r thist}
hist(columna, xlab = nombre.columna)
```
```{r tgr}
test.de.Grubbs <- grubbs.test(columna, two.sided = T)
print(test.de.Grubbs)
```
```{r tout}
indice.de.outlier.Grubbs <- which.max(abs(columna - mean(columna)))
valor.de.outlier.Grubbs <- columna[indice.de.outlier.Grubbs]
print(indice.de.outlier.Grubbs)
```
```{r tpl}
MiPlot_Univariate_Outliers(columna, indice.de.outlier.Grubbs, nombre.columna)
```
```{r tgrf}
MiPlot_resultados_TestGrubbs <- function(datos, alpha = 0.05) {
  test = grubbs.test(datos, two.sided = TRUE)
  
  if (test$p.value < alpha) {
    index = which.max(abs(datos - mean(datos)))
    valor = datos[index]
    
    cat(
      "Encontrado outlier con valor", valor, "en la posición", 
      index, "(p =", test$p.value, ")\n"
    )
    MiPlot_Univariate_Outliers(datos, index, "Test de Grubbs")
  } else {
    cat("No se han encontrado outliers (p =", test$p.value, ")\n")
  }
}
MiPlot_resultados_TestGrubbs(columna)
```

## Test de Rosner

```{r tros}
test.de.rosner <- rosnerTest(columna, k = 4)
cat(
  test.de.rosner$all.stats$Outlier, "\n",
  test.de.rosner$all.stats$Obs.Num
)
```
```{r trp}
MiPlot_Univariate_Outliers(
  columna,
  test.de.rosner$all.stats$Obs.Num[test.de.rosner$all.stats$Outlier],
  nombre.columna
)
```
```{r trf}
MiPlot_resultados_TestRosner <- function(datos, k) {
  test.de.rosner <- rosnerTest(datos, k = k)
  outliers.index <- test.de.rosner$all.stats$Obs.Num[test.de.rosner$all.stats$Outlier]
  outliers.value <- datos[outliers.index]
  cat(
    k, " mayores desviaciones de la media: ", test.de.rosner$all.stats$Obs.Num, "\n",
    "De estas, ¿quién es outlier? ", test.de.rosner$all.stats$Outlier, "\n",
    "Índices de los outliers: ", outliers.index, "\n",
    "Valores de los outliers: ", outliers.value, "\n",
    "Número de datos: ", length(datos), "\n", sep = ""
  )
  MiPlot_Univariate_Outliers(
    datos,
    outliers.index,
    "Test de Rosner"
  )
}

MiPlot_resultados_TestRosner(columna, 10)
```
-->

# Anomalías multivariantes: paquete mvoutlier

## Obtención de los outliers multivariantes
```{r mvpre, echo=F, include=F}
alpha.value <- 0.05
alpha.value.penalizado <- 1 - ( 1 - alpha.value) ^ (1 / nrow(mydata.numeric))
set.seed(12)
```

```{r mvmv, fig.width=10, out.width="\\textwidth"}
mvoutlier.plot <- mvoutlier::uni.plot(
  mydata.numeric.scaled, symb = F, alpha = alpha.value.penalizado
)
```

En el gráfico se observa que la mayoría de outliers multivariantes se sitúan por encima de la media en todas las variables.

## Análisis de los outliers

Extraemos los outliers multivariantes:

```{r mva1}
is.MCD.outlier <- mvoutlier.plot$outliers
numero.de.outliers.MCD <- sum(is.MCD.outlier)
print(numero.de.outliers.MCD)
```

Los outliers multivariantes resultan ser una mayor proporción de las instancias, en concreto forman un `r numero.de.outliers.MCD/nrow(abalone)*100`% de las mismas.

```{r mva2}
indices.de.outliers.en.alguna.columna  <-
  vector_claves_outliers_IQR_en_alguna_columna(mydata.numeric)
indices.de.outliers.multivariantes.MCD <- which(is.MCD.outlier)
indices.de.outliers.multivariantes.MCD.pero.no.1variantes <- setdiff(
  indices.de.outliers.multivariantes.MCD,
  indices.de.outliers.en.alguna.columna
)
nombres.de.outliers.multivariantes.MCD.pero.no.1variantes <-
  rownames(mydata.numeric)[indices.de.outliers.multivariantes.MCD.pero.no.1variantes]
```

Efectivamente, hay `r length(nombres.de.outliers.multivariantes.MCD.pero.no.1variantes)` instancias que son outliers multivariantes según la distancia de Mahalanobis pero no univariantes. Creamos un data.frame restringido a los outliers MCD (muestro los 10 primeros):

```{r mvdf, echo = 1}
data.frame.solo.outliers <- mydata.numeric.scaled[is.MCD.outlier, ]
print(data.frame.solo.outliers[1:10, ])
```

## Boxplot y biplot

```{r mvpl, out.width=".8\\textwidth", fig.width=8, fig.height=6, out.height=".6\\textwidth"}
MiBoxPlot_juntos(mydata.numeric, is.MCD.outlier)
MiBiPlot_Multivariate_Outliers(mydata.numeric, is.MCD.outlier, "Biplot")
```

El boxplot es díficil de evaluar dado que las etiquetas de los datos se solapan demasiado.

En el biplot se aprecia que los ejemplos que están fuera de la frontera exterior de la región determinada por las dos componentes principales se han considerado outliers multivariantes, pero también muchas instancias que están dentro de dicha frontera. Podríamos pensar que la distancia de Mahalanobis está marcando como outliers tanto una región que realmente son datos muy anómalos como otra región de datos que tal vez son escasos pero una parte más común de la población.

Es difícil apreciar qué variables están contribuyendo en mayor medida a los outliers multivariantes.

# Local Outlier Factor

## Lectura y preprocesamiento

Seleccionamos 4 de las 8 variables númericas que tengan menos relación entre sí (*height* es de tamaño, *whole* y *shell* de peso y *rings* el número de anillos).

```{r nspre}
mis.datos.numericos <- mydata.numeric[, c("height", "whole", "shell", "rings")]
mis.datos.numericos.normalizados <- as.data.frame(lapply(mis.datos.numericos, scale))
rownames(mis.datos.numericos.normalizados) <- rownames(mis.datos.numericos)
```

## Outliers basados en distancias

Establecemos una comparación con 5 vecinos para LOF y obtenemos las scores:

```{r lof1, fig.width=8, out.width=".8\\textwidth"}
numero.de.vecinos.lof <- 5
lof.scores <- lofactor(mis.datos.numericos.normalizados, numero.de.vecinos.lof)
plot(sort(lof.scores))
```

Observamos 2 valores más altos que el resto y trabajaremos con estos, los extraemos ordenando la score y extrayendo los dos índices superiores:

```{r lofs}
numero.de.outliers <- 2
indices.de.lof.outliers.ordenados <- order(lof.scores, decreasing = T)
indices.de.lof.top.outliers <- indices.de.lof.outliers.ordenados[1:numero.de.outliers]
print(indices.de.lof.top.outliers)
is.lof.outlier <- 1:nrow(mis.datos.numericos) %in% indices.de.lof.top.outliers
```

```{r lofpl}
MiBiPlot_Multivariate_Outliers(mis.datos.numericos, is.lof.outlier, "LOF outliers") 
```

En el gráfico se observa que los dos datos que se han marcado como outliers están muy distanciados de la región condensada de datos. Las variables *height*, *whole* y *shell* están bastante correladas por lo que aparecen en direcciones similares en el biplot, y es la dirección en que se separan los outliers del cluster central. La variable *rings* no afecta notablemente a estos outliers.

## Comparación con una sola variable

```{r lofcmp}
vector.claves.outliers.IQR.en.alguna.columna <- 
  vector_claves_outliers_IQR_en_alguna_columna(mis.datos.numericos)
vector.es.outlier.IQR.en.alguna.columna <- 
  vector_es_outlier_IQR_en_alguna_columna(mis.datos.numericos)

MiBiPlot_Multivariate_Outliers(
  mis.datos.numericos, 
  vector.es.outlier.IQR.en.alguna.columna, 
  "outliers iqr en alguna columna"
)
```

Este biplot nos muestra que los outliers calculados con IQR en alguna columna se ubican tanto bordeando el clúster central de datos como ocupando cierta región poco densa del mismo. Además, los dos outliers detectados mediante LOF son también de tipo IQR en alguna columna (en vista de la evidencia anterior, lo serán probablemente en *height*):

```{r lofc2}
indices.de.outliers.multivariantes.LOF.pero.no.1variantes <- setdiff(
  indices.de.lof.top.outliers, 
  vector.claves.outliers.IQR.en.alguna.columna
)
cat(indices.de.outliers.multivariantes.LOF.pero.no.1variantes)
```

## Ampliación: Filtro automático de columnas numéricas

Construimos automáticamente un data.frame con las columnas numéricas de otro:

```{r lofamp}
select_numeric <- function(datos) {
  datos[, sapply(datos, is.numeric)]
}

str(select_numeric(abalone))
```

# Métodos basados en clustering

Puesto que aparentemente los datos forman una región densa con algunas zonas adyacentes más dispersas, los métodos basados en agrupamiento probablemente no sean los más adecuados para detectar outliers en este caso. Establezco el número de clusters a buscar a 3, de forma que posiblemente alguno de los clusters pueda salir de la región densa.

```{r clpre}
numero.de.outliers   = 2
numero.de.clusters   = 3
set.seed(2)
```

## Distancia euclídea con k-means

Extraigo el modelo de k-means y genero un biplot coloreando los clusters:

```{r kmmod}
modelo.kmeans <- kmeans(mis.datos.numericos.normalizados, centers = numero.de.clusters)
indices.clustering <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

BIPLOT.isOutlier             = FALSE
BIPLOT.cluster.colors        = colorspace::rainbow_hcl(numero.de.clusters)
BIPLOT.asignaciones.clusters = indices.clustering
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers")
```

Observamos que los 3 clusters generados cubren buena parte de la región densa de datos. Puede ser una consecuencia de que k-means no es robusto ante diferencias de densidad en clusters, y por tanto tiende a centrar los grupos en los lugares donde hay más instancias.

Calculamos las distancias a los centroides mediante la función `distancias_a_centroides` y seleccionamos los outliers más destacables:

```{r, include=F}
distancias_a_centroides <- function(
  datos.normalizados,
  indices.asignacion.clustering, 
  datos.centroides.normalizados) {
  sqrt(rowSums(
    (datos.normalizados - datos.centroides.normalizados[indices.asignacion.clustering,])^2
  ))
}
```

```{r kmdis}
dist.centroides <- distancias_a_centroides(
  mis.datos.numericos.normalizados,
  indices.clustering,
  centroides.normalizados
)

top.outliers <- order(dist.centroides, decreasing = T)[1:numero.de.outliers]
print(top.outliers)
```

Vemos que según las distancias a los centroides, encontramos los mismos 2 outliers que con LOF. Recogemos este proceso en una función y obtenemos tanto estos outliers como las distancias a sus centroides:

```{r kmtop}
top_clustering_outliers <- function(
  datos.normalizados,
  indices.asignacion.clustering,
  datos.centroides.normalizados,
  numero.de.outliers
) {
  dist.centroides <- distancias_a_centroides(
    datos.normalizados,
    indices.asignacion.clustering,
    datos.centroides.normalizados
  )

  top.outliers <- order(dist.centroides, decreasing = T)[1:numero.de.outliers]

  list(
    indices = top.outliers,
    distancias = dist.centroides[top.outliers]
  )
}
```

```{r kmf}
print(top_clustering_outliers(
  mis.datos.numericos.normalizados,
  indices.clustering,
  centroides.normalizados, 
  numero.de.outliers
))
```

Recuperamos los valores de los centroides en las escalas iniciales:

```{r kmdez}
mis.datos.medias <- colMeans(mis.datos.numericos)
mis.datos.desviaciones <- sapply(mis.datos.numericos, sd)
centroides.valores <- 
  centroides.normalizados %>%
  sweep(2, mis.datos.desviaciones, "*") %>%
  sweep(2, mis.datos.medias, "+")
print(centroides.valores)
```

Vemos que cada centroide se ha centrado en un "rango" de los datos, en cuanto a que el primero englobará individuos de altura, peso y edad medios; el segundo abarcará los individuos mayores en tamaño, peso y edad, y el tercero comprende a los pequeños y ligeros. Sin embargo, estos tres grupos no tienen una separación presente en los datos, sino artificial.

## Ampliación: Distancia euclídea con *Partition around medoids*

Construimos el modelo de *Partition around medoids* y obtenemos los centroides (que son datos de nuestro dataset) tanto en escala normalizada como sin normalizar:

```{r pam1}
modelo.pam <- 
  mis.datos.numericos.normalizados %>%
  dist %>%
  cluster::pam(numero.de.clusters)

indices.clustering.pam <- modelo.pam$clustering
centroides.normalizados.pam <- mis.datos.numericos.normalizados[modelo.pam$medoids, ]
print(centroides.normalizados.pam)
print(mis.datos.numericos[modelo.pam$medoids, ])
```

Vemos que este método encuentra en este caso los mismos outliers que k-means y a distancias similares de los centroides:

```{r pam2}
print(top_clustering_outliers(
  mis.datos.numericos.normalizados,
  indices.clustering.pam,
  centroides.normalizados.pam, 
  numero.de.outliers
))
```

En parte, esto es consecuencia de la abundancia de datos en la región densa del dataset. Los medoides encontrados son muy similares a los centroides de k-means, luego los resultados no pueden variar mucho.

## Ampliación: Distancia de Mahalanobis

Calculamos los outliers más destacables según la distancia de Mahalanobis al centroide. Para ello, definimos una función y la usamos:

```{r dmc, fig.width=10, out.width="\\textwidth", fig.height=7, out.height=".7\\textwidth"}
top_clustering_outliers_distancia_mahalanobis <- function(
  datos, 
  indices.asignacion.clustering, 
  numero.de.outliers
) {
  cluster.ids = unique(indices.asignacion.clustering)
  k           = length(cluster.ids)
  seleccion   = sapply(1:k, function(x) indices.asignacion.clustering == x)
  
  # Usando la estimación robusta de la media y covarianza: (cov.rob del paquete MASS)
  lista.matriz.de.covarianzas = lapply(1:k, function(x)
    cov.rob(mis.datos.numericos[seleccion[, x], ])$cov)
  lista.vector.de.medias = lapply(1:k, function(x)
    cov.rob(mis.datos.numericos[seleccion[, x], ])$center)
    
  mah.distances = lapply(1:k, function(x) mahalanobis(
    mis.datos.numericos[seleccion[,x],], 
    lista.vector.de.medias[[x]], 
    lista.matriz.de.covarianzas[[x]]
  ))  
  
  todos.juntos = unlist(mah.distances)
  seleccionados = order(todos.juntos, decreasing=TRUE)[1:numero.de.outliers]
  indices.top.mah.outliers = as.numeric(names(todos.juntos[seleccionados]))
  
  list(
    indices = indices.top.mah.outliers,
    distancias = todos.juntos[seleccionados]
  )
}

top.clustering.outliers.mah = top_clustering_outliers_distancia_mahalanobis(
  mis.datos.numericos, 
  indices.clustering, 
  numero.de.outliers
)
print(top.clustering.outliers.mah)

is.kmeans.outlier.mah <-
  1:nrow(mis.datos.numericos) %in% top.clustering.outliers.mah$indices

BIPLOT.isOutlier             = is.kmeans.outlier.mah
BIPLOT.cluster.colors        = colorspace::rainbow_hcl(numero.de.clusters)
BIPLOT.asignaciones.clusters = indices.clustering
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers")
```

Es interesante notar que la distancia de Mahalanobis detecta uno de los outliers que se detectaban con la distancia euclídea (de índice 2052), pero el segundo (3997) no se había detectado antes. Además, el biplot no muestra claramente la razón de que este sea un outlier, es posible que las dos componentes principales no sean suficientes para mostrar que este ejemplo está lejos de su centroide según la distancia de Mahalanobis.


## Ampliación: Distancia relativa

```{r rel}
top_clustering_outliers_distancia_relativa = function(
  datos.normalizados, 
  indices.asignacion.clustering, 
  datos.centroides.normalizados, 
  numero.de.outliers
) {
  dist_centroides = distancias_a_centroides(
    datos.normalizados, 
    indices.asignacion.clustering, 
    datos.centroides.normalizados
  )
  
  cluster.ids = unique(indices.asignacion.clustering)
  k           = length(cluster.ids)
  
  distancias.a.centroides.por.cluster = sapply(1:k, function(x) 
    dist_centroides[indices.asignacion.clustering  == cluster.ids[x]]
  )
  
  distancias.medianas.de.cada.cluster = sapply(1:k, function(x) 
    median(dist_centroides[[x]])
  )
  
  todas.las.distancias.medianas.de.cada.cluster = 
    distancias.medianas.de.cada.cluster[indices.asignacion.clustering]
  ratios = dist_centroides / todas.las.distancias.medianas.de.cada.cluster
  
  indices.top.outliers = order(ratios, decreasing=T)[1:numero.de.outliers]
  
  list(
    distancias = ratios[indices.top.outliers],
    indices = indices.top.outliers
  )
}

top.outliers.kmeans.distancia.relativa = top_clustering_outliers_distancia_relativa(
  mis.datos.numericos.normalizados,
  indices.clustering, 
  centroides.normalizados, 
  numero.de.outliers
)
print(top.outliers.kmeans.distancia.relativa)
```

En este caso, la distancia relativa sí encuentra los mismos outliers que detectábamos con la euclídea.

# Resumen de resultados

El conjunto de datos *abalone* que se ha analizado presenta datos anómalos de dos tipos: hemos comprobado que hay dos ejemplos con una *height* muy superior a lo normal, que posiblemente se deban a errores de medición, y por otro lado hay una región de datos bastante poco densa que corresponde con ejemplares muy longevos. Los outliers del primer tipo los hemos encontrado con Local Outlier Factor y al hacer boxplots sobre el atributo *height*, así como con los métodos basados en clustering. Los outliers del segundo tipo los hemos estudiado buscando anomalías univariantes sobre la variable *rings*.

Mediante el análisis de outliers multivariantes con la distancia de Mahalanobis, hemos encontrado bastantes ejemplos que no eran outliers univariantes pero sí eran multivariantes. Esto se puede deber a discordancias entre variables, por ejemplo, un espécimen muy grande y poco pesado o muy pequeño pero de mucha edad. Sin embargo, la enorme cantidad de outliers encontrados por este método ha impedido un análisis caso a caso.

Por último, los métodos no supervisados basados en clustering podrían haber descartado outliers si hubiesen considerado la zona densa y la zona poco densa como clusters distintos, pero los métodos aplicados no han conseguido este objetivo. Un algoritmo basado en densidad como DBSCAN podría haber encontrado un solo cluster denso y haber considerado el resto de datos como ruidosos, un resultado que habría sido más razonable.

\subsection*{Nota sobre reproducibilidad}

Este trabajo se ha redactado en un formato enteramente reproducible, y se encuentra disponible en un repositorio de código abierto: <https://github.com/fdavidcl/data-science/blob/master/unsupervised/practicas/anomalias.rmd>. Para ejecutarlo, son necesarios los ficheros `!Outliers_A2_Librerias_a_cargar_en_cada_sesion.R` y `!Outliers_A3_Funciones_a_cargar_en_cada_sesion.R`, así como el paquete `magrittr`.